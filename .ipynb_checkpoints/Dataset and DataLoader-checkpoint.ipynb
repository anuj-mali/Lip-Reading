{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a29ad6e-0626-4ab0-9ccf-d627f302fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd212312-081e-4b61-b280-9510665d8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aefaa3a-8e8d-4045-8b3f-7966f9128349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bfbf9e-af7f-4643-973a-0be4dc0487be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile modular/utils.py\n",
    "\n",
    "import os\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import numpy as np\n",
    "\n",
    "landmarks_dir = \"./landmarks/\"\n",
    "\n",
    "# Path of landmarks model\n",
    "landmarks_path = os.path.join(landmarks_dir + \"/shape_predictor_68_face_landmarks_GTX.dat\")\n",
    "\n",
    "# Create a face detector\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Create a landmark detector\n",
    "landmark_detector = dlib.shape_predictor(landmarks_path)\n",
    "\n",
    "def get_landmarks(frame) -> np.ndarray:\n",
    "    \"\"\"Takes a frame and generates landmarks for the first face\n",
    "\n",
    "        Args:\n",
    "            frame: video frame or image required to generate landmarks\n",
    "        \n",
    "        Returns:\n",
    "            A numpy array containing the co-ordinates of the landmarks of the first face in the given frame\n",
    "    \"\"\"\n",
    "    faces = face_detector(frame)\n",
    "    landmarks = None\n",
    "    if faces:\n",
    "        landmarks = landmark_detector(frame, faces[0])\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "    return landmarks\n",
    "\n",
    "def generate_video_landmarks(frames) -> np.ndarray:\n",
    "    \"\"\"Generate landmarks the given video\n",
    "    \n",
    "        Args:\n",
    "            filename (str): filename specifying the video\n",
    "    \n",
    "        Returns:\n",
    "            A numpy.ndarray containing all the landmarks for the faces in the video\"\"\"\n",
    "    landmarks = []\n",
    "    for frame in frames:\n",
    "        landmarks.append(get_landmarks(frame))\n",
    "    # landmarks = landmarks_interpolation(landmarks)\n",
    "    return np.array(landmarks)\n",
    "\n",
    "def landmarks_interpolation(landmarks):\n",
    "    \"\"\"Adds the missing landmarks to the landmarks array\n",
    "\n",
    "    Args:\n",
    "        landmarks: An array containing all the detected landmarks\n",
    "\n",
    "    Returns:\n",
    "        landmarks array filled in with missing landmarks\n",
    "    \"\"\"\n",
    "    # Obtain indices of all the valid landmarks (i.e landmarks not None)\n",
    "    valid_landmarks_idx = [idx for idx, _ in enumerate(landmarks) if _ is not None]\n",
    "    \n",
    "    if not valid_landmarks_idx:\n",
    "        return\n",
    "\n",
    "    # For middle parts of the landmarks array\n",
    "    for idx in range(1, len(valid_landmarks_idx)):\n",
    "        # If the valid landmarks indices are adjacent then skip to next iteration\n",
    "        if valid_landmarks_idx[idx]-valid_landmarks_idx[idx-1] > 1:\n",
    "            landmarks = linear_interpolation(start_idx=valid_landmarks_idx[idx-1],\n",
    "                                            end_idx=valid_landmarks_idx[idx],\n",
    "                                            landmarks=landmarks)\n",
    "\n",
    "    # For beginning and ending parts of the landmarks array\n",
    "    valid_landmarks_idx = [idx for idx, _ in enumerate(landmarks) if _ is not None]\n",
    "    landmarks[:valid_landmarks_idx[0]] = [landmarks[valid_landmarks_idx[0]]] * valid_landmarks_idx[0]\n",
    "    landmarks[valid_landmarks_idx[-1]:] = [landmarks[valid_landmarks_idx[-1]]] * (len(landmarks) - valid_landmarks_idx[-1])\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "def linear_interpolation(start_idx: int, end_idx: int, landmarks):\n",
    "    \"\"\"Defines a linear interpolation function to interpolate missing landmarks between indices\n",
    "\n",
    "    Args:\n",
    "        start_idx (int): An integer defining the starting index\n",
    "        end_idx (int): An integer defining the stopping index\n",
    "        landmarks: An array of size 68 containing the (x,y) values of the facial landmarks\n",
    "\n",
    "    Returns:\n",
    "        landmarks array after the missing points have been interpolated.\n",
    "    \"\"\"\n",
    "    start_landmarks = landmarks[start_idx]\n",
    "    end_landmarks = landmarks[end_idx]\n",
    "    delta_idx = end_idx - start_idx\n",
    "    delta_landmarks = end_landmarks - start_landmarks\n",
    "\n",
    "    # Apply linear interpolation formula\n",
    "    for idx in range(1, delta_idx):\n",
    "        landmarks[idx + start_idx] = start_landmarks + delta_landmarks/delta_idx * idx\n",
    "    return landmarks\n",
    "\n",
    "def find_classes(dataframe: pd.DataFrame) -> Tuple[List, Dict[str, int]]:\n",
    "    class_names = dataframe['label'].unique()\n",
    "    class_to_idx = {label: idx for idx, label in enumerate(class_names)}\n",
    "    return class_names, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54b1111-8e9d-4339-b233-decbccd19994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.io import read_video\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, transform=None, landmarks_dir: str =\"landmarks\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.class_names, self.class_to_idx = find_classes(dataframe)\n",
    "       \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns total number of paths in dataframe\"\"\"\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.dataframe.iloc[idx]['path']\n",
    "        frames,_,_ = read_video(filename=str(path), pts_unit='sec', output_format='TCHW')\n",
    "        landmarks = generate_video_landmarks(frames.permute(0,2,3,1).numpy())\n",
    "\n",
    "        if landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        class_name = self.dataframe.iloc[idx]['label']\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames, landmarks)\n",
    "        return frames,class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eaae4c-4f55-4a75-9c3b-76f0ab53975a",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a2eb86-59b8-4f28-b768-07fc5333c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "class WarpAffine(nn.Module):\n",
    "    def __init__(self, mean_landmarks_path: str):\n",
    "        super().__init__()\n",
    "        self.mean_landmarks = np.load(mean_landmarks_path)\n",
    "        self.stable_points=(28, 33, 36, 39, 42, 45, 48, 54)\n",
    "        self.reference_landmarks = np.vstack([self.mean_landmarks[x] for x in self.stable_points])\n",
    "\n",
    "    def get_transform(self, src_points: np.ndarray) -> np.ndarray:\n",
    "        transform, _ = cv2.estimateAffinePartial2D(src_points, self.reference_landmarks, method=cv2.LMEDS)\n",
    "        return transform\n",
    "        \n",
    "    \n",
    "    def forward(self, frames, landmarks_list):\n",
    "        warped_frames = []\n",
    "        transformed_landmarks_list = []\n",
    "        for i, landmarks in enumerate(landmarks_list):\n",
    "            # Extract only stable points from current landmarks\n",
    "            stable_points_landmarks = np.vstack([landmarks[x] for x in self.stable_points])\n",
    "    \n",
    "            # Calculate affine transformation matrix\n",
    "            transform_matrix = self.get_transform(stable_points_landmarks)\n",
    "    \n",
    "            # Apply affine transformation to each frame\n",
    "            h, w = frames[i].shape[1:]\n",
    "            warped_frame = cv2.warpAffine(frames[i].squeeze().numpy(), transform_matrix, dsize=(256,256))\n",
    "            transformed_landmarks = np.matmul(landmarks, transform_matrix[:, :2].transpose()) + transform_matrix[:,2].transpose() \n",
    "    \n",
    "            warped_frames.append(warped_frame)\n",
    "            transformed_landmarks_list.append(transformed_landmarks)\n",
    "\n",
    "        return np.array(warped_frames), np.array(transformed_landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0dbd63a-d3e2-4411-8e3f-4ee4161800c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothLandmarks(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temporal_window = 6\n",
    "        \n",
    "    def forward(self, frames, landmarks_list, smoothed_landmarks_list=None):\n",
    "        if smoothed_landmarks_list is None:\n",
    "            smoothed_landmarks_list = []\n",
    "        for idx, landmarks in enumerate(landmarks_list):\n",
    "            window_margin = min(self.temporal_window, idx, len(landmarks_list) - 1 - idx)\n",
    "            smoothed_landmarks = np.mean([landmarks_list[x] for x in range(idx - window_margin, idx + window_margin + 1)], axis=0)\n",
    "            smoothed_landmarks += landmarks_list[idx].mean(axis=0) - smoothed_landmarks.mean(axis=0)\n",
    "            smoothed_landmarks_list.append(smoothed_landmarks)\n",
    "            \n",
    "        return frames, np.array(smoothed_landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a32c17c-9cf4-454a-b4b2-8d783cab665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropPatch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.crop_height = 96\n",
    "        self.crop_width = 96\n",
    "        self.start_idx, self.end_idx = 48,68\n",
    "    \n",
    "    def forward(self, frames, landmarks_list):\n",
    "        cropped_landmarks = landmarks_list[:, self.start_idx:self.end_idx, :]\n",
    "        center = np.round(np.mean(cropped_landmarks, axis=1))\n",
    "        center_x, center_y = center[:,0].astype(int), center[:,1].astype(int)\n",
    "\n",
    "        mouth_roi = np.zeros((len(frames), self.crop_height, self.crop_width))\n",
    "\n",
    "        for i, frame in enumerate(frames):\n",
    "            mouth_roi[i] = frame[center_y[i]-self.crop_height//2:self.crop_height//2+center_y[i], center_x[i]-self.crop_width//2 : self.crop_width//2+center_x[i]]\n",
    "        \n",
    "        return torch.tensor(mouth_roi, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bf1ab2-fa4d-4cb1-9547-b8c304b85c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "data_transforms = v2.Compose([\n",
    "    v2.Grayscale(),\n",
    "    v2.Resize(size=(256,256), antialias=False),\n",
    "    SmoothLandmarks(),\n",
    "    WarpAffine(mean_landmarks_path=\"mean_landmarks/20words_mean_face.npy\"),\n",
    "    CropPatch(),\n",
    "])\n",
    "train_data = VideoDataset(dataframe=pd.read_pickle(\"dataframe/train_df_3.pkl\"), transform=data_transforms)\n",
    "test_data = VideoDataset(dataframe=pd.read_pickle(\"dataframe/test_df_3.pkl\"), transform=data_transforms)\n",
    "val_data = VideoDataset(dataframe=pd.read_pickle(\"dataframe/val_df_3.pkl\"), transform=data_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966cf55-e781-4ff8-9c49-5f7e39eb66a1",
   "metadata": {},
   "source": [
    "## Testing Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b14027-06de-4aa3-901c-2150daa5483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "transform_test = v2.Compose([\n",
    "    v2.Grayscale(),\n",
    "    v2.Resize(size=(256,256), antialias=False),\n",
    "    SmoothLandmarks(),\n",
    "    WarpAffine(mean_landmarks_path=\"mean_landmarks/20words_mean_face.npy\"),\n",
    "    CropPatch(),\n",
    "    # v2.ToDtype(torch.float32),\n",
    "    # ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14704ff-e9e0-4de9-a12b-6292e44b6d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m frames, landmarks,class_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m275803\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 15\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m     frames,_,_ \u001b[38;5;241m=\u001b[39m read_video(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(path), pts_unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m'\u001b[39m, output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTCHW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m generate_video_landmarks(frames\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/pandas/core/indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1714\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/pandas/core/indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1645\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "frames, landmarks,class_idx = train_data[275803]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2805ed-129a-42b1-b802-66416ad69ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transform_test(frames, landmarks)\n",
    "type(transformed)\n",
    "transformed.dtype\n",
    "# center.shape\n",
    "# print(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e9ba2-327f-45d5-9fa5-42ab3237fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c13e1-6bc8-47db-87a9-7f73181e4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frames, _,_ = read_video(\"./data/lipread_mp4/ABOUT/train/ABOUT_00002.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835eb349-8c83-4f4e-bd45-022ecd5984ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_landmarks = generate_video_landmarks(test_frames.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819069e-29d7-4cdf-87f1-8e5453b4ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632157c5-d44f-462d-baa3-7dc445dce8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_landmarks = generate_video_landmarks(transformed.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec911a11-42ce-4925-a44c-c8e8befc2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ea2ab-0e5c-4408-b862-13718bffbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(transformed[20], cmap='gray')\n",
    "mean_landmarks_np = np.load('mean_landmarks/20words_mean_face.npy')\n",
    "plt.scatter(mean_landmarks_np[:,0], mean_landmarks_np[:, 1], label=\"Transformed\")\n",
    "plt.scatter(t_landmarks[20][:,0], t_landmarks[20][:,1], alpha=0.4)\n",
    "plt.scatter(center[20][0], center[20][1],alpha=1)\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(test_landmarks[20][:,0], test_landmarks[20][:,1],  label=\"Original\")\n",
    "plt.imshow(test_frames[20], cmap='gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(transformed[20], cmap='gray')\n",
    "plt.scatter(transformed_landmarks[20][:,0], transformed_landmarks[20][:,1])\n",
    "plt.scatter(t_landmarks[20][:,0], t_landmarks[20][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563da326-b320-4888-a573-ff5a276c155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, _,_ = train_data[275803]\n",
    "\n",
    "test_frames, _,_ = read_video(\"./data/lipread_mp4/ABOUT/train/ABOUT_00002.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae1bd75-cfbf-469b-b99b-711314495ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frames[20])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_frames[20].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e953b-6309-4009-a0aa-086693fff5bb",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bf02fb-12a7-4517-8e30-378cc55321b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import os\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def my_collate(batch):\n",
    "    len_batch = len(batch) # original batch length\n",
    "    batch = list(filter (lambda x:x is not None, batch)) # filter out all the Nones\n",
    "    if len_batch > len(batch): # source all the required samples from the original dataset at random\n",
    "        diff = len_batch - len(batch)\n",
    "        for i in range(diff):\n",
    "            batch.append(dataset[np.random.randint(0, len(dataset))])\n",
    "\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=True,\n",
    "                             pin_memory=True,\n",
    "                             collate_fn=my_collate)\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,\n",
    "                            collate_fn=my_collate)\n",
    "val_dataloader = DataLoader(dataset=val_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True,\n",
    "                           collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c367f3-dffd-439b-b29e-2c7e217ef8ff",
   "metadata": {},
   "source": [
    "## Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84f5cb7f-d12c-4ffe-b2f3-a18395481e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10a24a-84d9-4a71-8d41-8c7c1c22d53e",
   "metadata": {},
   "source": [
    "## Train and Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7fde5b-1766-4ba4-82db-99fd21e31224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              device: torch.device=device):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        if batch % 20 == 0:\n",
    "            print(batch/len(dataloader) * 100)\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_logits = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer Step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_logits)\n",
    "\n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    train_acc = train_acc/len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc77858-c38e-4bb4-bc74-3ec811ed93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device=device):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch,(X,y) in enumerate(dataloader):\n",
    "            X,y = X.to(device), y.to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            y_logits = model(X)\n",
    "    \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(y_logits, y)\n",
    "            val_loss += loss\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            val_labels = y_logits.argmax(dim=1)\n",
    "            val_acc += ((val_labels == y).sum().item()/len(val_labels))\n",
    "\n",
    "    val_loss = val_loss/len(dataloader)\n",
    "    val_acc = val_acc/len(dataloader)\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fedb380-ec15-490c-8da3-dd9f6a66f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "         train_dataloader: torch.utils.data.DataLoader,\n",
    "         val_dataloader: torch.utils.data.DataLoader,\n",
    "         loss_fn: torch.nn.Module,\n",
    "         optimizer: torch.optim.Optimizer,\n",
    "         epochs: int = 32,\n",
    "         device: torch.device = device):\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                dataloader=train_dataloader,\n",
    "                                loss_fn=loss_fn,\n",
    "                                optimizer=optimizer,\n",
    "                                device=device)\n",
    "        val_loss, val_acc = val_step(model=model,\n",
    "                           dataloader=val_dataloader,\n",
    "                           loss_fn=loss_fn,\n",
    "                           device=device)\n",
    "\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.2f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8321952-88d1-4663-b358-dd3b8d9ce847",
   "metadata": {},
   "source": [
    "## Baseline Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e67916-b0ca-4427-a614-7cd1b36b3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNonLinearModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08eb271c-0f64-4d81-a252-8232c9637f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.class_names\n",
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb80417a-fc41-4570-98c2-459ed8884643",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineNonLinearModel(input_shape=29*96*96, hidden_units=128, output_shape=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bd64879-c18b-49ac-9e89-3aded8cf5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=baseline_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20497590-efee-486a-98c4-bb970ff9cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                     | 0/2 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m      2\u001b[0m          train_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[1;32m      3\u001b[0m          val_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m          epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      7\u001b[0m          device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 10\u001b[0m         train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         val_loss, val_acc \u001b[38;5;241m=\u001b[39m val_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m                            dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[1;32m     17\u001b[0m                            loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     18\u001b[0m                            device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/Project/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "train(model=baseline_model,\n",
    "     train_dataloader=train_dataloader,\n",
    "     val_dataloader=val_dataloader,\n",
    "     loss_fn=loss_fn,\n",
    "     optimizer=optimizer,\n",
    "     device=device,\n",
    "     epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd841de5-29de-43af-b2e6-ddc984624763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 257 ms, sys: 182 Âµs, total: 257 ms\n",
      "Wall time: 257 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 69,  78],\n",
       "        [ 68,  95],\n",
       "        [ 69, 114],\n",
       "        ...,\n",
       "        [139, 159],\n",
       "        [132, 159],\n",
       "        [125, 157]],\n",
       "\n",
       "       [[ 68,  85],\n",
       "        [ 67, 102],\n",
       "        [ 68, 120],\n",
       "        ...,\n",
       "        [140, 159],\n",
       "        [133, 159],\n",
       "        [127, 158]],\n",
       "\n",
       "       [[ 68,  71],\n",
       "        [ 67,  90],\n",
       "        [ 66, 109],\n",
       "        ...,\n",
       "        [138, 158],\n",
       "        [130, 159],\n",
       "        [123, 157]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 81,  55],\n",
       "        [ 75,  74],\n",
       "        [ 69,  93],\n",
       "        ...,\n",
       "        [134, 161],\n",
       "        [127, 159],\n",
       "        [121, 154]],\n",
       "\n",
       "       [[ 83,  54],\n",
       "        [ 77,  73],\n",
       "        [ 71,  92],\n",
       "        ...,\n",
       "        [136, 161],\n",
       "        [128, 159],\n",
       "        [121, 155]],\n",
       "\n",
       "       [[ 82,  56],\n",
       "        [ 76,  75],\n",
       "        [ 71,  94],\n",
       "        ...,\n",
       "        [136, 161],\n",
       "        [129, 160],\n",
       "        [122, 156]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frames, _,_ = read_video(\"./data/lipread_mp4/ABOUT/train/ABOUT_00002.mp4\")\n",
    "\n",
    "generate_video_landmarks(test_frames.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919582d-1c02-4f70-a236-56f29fc5f9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
